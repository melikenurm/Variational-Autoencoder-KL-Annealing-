{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "peHcnU5kipDQ",
    "outputId": "31cb7f49-b725-45d3-ae3e-c332320da85d"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generative adversarial network (GAN) for a 1D normal distribution, based on\n",
    "    http://blog.evjang.com/2016/06/generative-adversarial-nets-in.html\n",
    "    http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/\n",
    "Original paper:\n",
    "    Generative adversarial networks.\n",
    "    https://arxiv.org/abs/1406.2661\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5_24s0l0zt7"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "class DataDistribution2d(object):\n",
    "    def __init__(self, mu=([2,6]), sigma=([[.25, 1], [.5,.25]])):\n",
    "        self.mu    = np.array(mu)\n",
    "        self.sigma = np.array(sigma)\n",
    "\n",
    "    def change(self, **kwargs):\n",
    "      self.mu=np.array(kwargs.get('mu'))\n",
    "      self.sigma=np.array(kwargs.get('sigma'))\n",
    "\n",
    "    def sample(self, size): #bu dağılımdan 'size' tane örnek üret\n",
    "        x = np.random.multivariate_normal(self.mu, self.sigma, size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AG4J4iPKix5K"
   },
   "outputs": [],
   "source": [
    "# Model - Fake data dist.\n",
    "class NoiseDistribution(object):\n",
    "    def __init__(self, mu=0, sigma=1):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def sample(self, size, sort=False): #noise dağılımımız (-bound, +bound) aralığında uniform gaussian hale getirebiliriz.\n",
    "        x = np.random.normal(self.mu, self.sigma, size) #+ np.random.random(size) * 0.01 #gaussian noise\n",
    "        if sort:\n",
    "            x = np.sort(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzSa4wT3YhZ4"
   },
   "outputs": [],
   "source": [
    "# Model - Fake data dist.\n",
    "class NoiseDistribution2d(object):\n",
    "    def __init__(self, mu=([1,2]), sigma=([[.25, 1], [.5,.25]])):\n",
    "        self.mu    = np.array(mu)\n",
    "        self.sigma = np.array(sigma)\n",
    "\n",
    "    def change(self, **kwargs):\n",
    "      self.mu=np.array(kwargs.get('mu'))\n",
    "      self.sigma=np.array(kwargs.get('sigma'))\n",
    "\n",
    "    def sample(self, size):\n",
    "        x = np.random.multivariate_normal(mean=self.mu, cov=self.sigma, size=size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-JYhxlKi4UR"
   },
   "outputs": [],
   "source": [
    "def weight(shape): #shape=nöron sayısı\n",
    "    bound = np.sqrt(6.0 / np.sum(shape))\n",
    "    init  = tf.random_uniform_initializer(-bound, bound) #ağırlıkların başlangıç değerleri rastgele uniform dağılımdan, init=tf.random_normal_initializer(stddev=1.0) for gaussian\n",
    "    return tf.get_variable('W', shape, initializer=init)\n",
    "\n",
    "def bias(shape):\n",
    "    init = tf.constant_initializer(0.0) #biasın başlangıç değeri 0\n",
    "    return tf.get_variable('b', shape, initializer=init)\n",
    "\n",
    "def linear(name, x, dim): #dim=shape\n",
    "    with tf.variable_scope(name):\n",
    "        W = weight([x.get_shape()[-1].value, dim])\n",
    "        b = bias([dim])\n",
    "    return tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2n_QDSYCA9L"
   },
   "outputs": [],
   "source": [
    "def generator2d(x, hidden_dim=4):\n",
    "    x = linear('hidden_1', x, hidden_dim)\n",
    "    x = tf.nn.softplus(x) # softplus = ln(1+e^x) şeklinde aktivasyon fonksiyonu \n",
    "    x = linear('hidden_2', x, hidden_dim) #2. layer'ı ekleyince kaymaya başlıyor(atlama yapıyor)\n",
    "    x = tf.nn.softplus(x)\n",
    "    x = linear('output', x, 2)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pYrRPNoCGZN"
   },
   "outputs": [],
   "source": [
    "def discriminator2d(x, hidden_dim=8):\n",
    "    x = linear('hidden_1', x, hidden_dim)\n",
    "    x = tf.nn.relu(x) # tanh(x) olabilir\n",
    "    x = linear('hidden_2', x, hidden_dim)\n",
    "    x = tf.nn.relu(x)\n",
    "    # 3. hidden layer eklenebilir \n",
    "    x = linear('logits', x, 1)\n",
    "    x = tf.sigmoid(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tCou6vki_fo"
   },
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "def get_train_op(loss, variables, initial_learning_rate,\n",
    "                 decay=0.96, decay_steps=200):\n",
    "    # Implement exponential decay of learning rate\n",
    "    global_step   = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        initial_learning_rate,\n",
    "        global_step,\n",
    "        decay_steps, #200 epochda bir adım miktarını küçült\n",
    "        decay,\n",
    "        staircase=True\n",
    "        )\n",
    "\n",
    "    # Note that we restrict to subset of variables\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate) #optimizasyon adam olabilir, subset değil de tüm variablelarla\n",
    "    train_op  = optimizer.minimize(loss, global_step, variables)\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bygFSXylKTPy"
   },
   "outputs": [],
   "source": [
    "# Adam\n",
    "def optimizer(loss, var_list, initial_learning_rate, decay=0.96, decay_steps=500): #decay_steps=500\n",
    "    step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        initial_learning_rate,\n",
    "        step,\n",
    "        decay_steps, #200 epochda bir adım miktarını küçült\n",
    "        decay,\n",
    "        staircase=True\n",
    "        )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "        loss,\n",
    "        global_step=step,\n",
    "        var_list=var_list\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCyFSwydjCbc"
   },
   "outputs": [],
   "source": [
    "def get_variables(scope):\n",
    "    return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4XlPo6KakSJL"
   },
   "outputs": [],
   "source": [
    "def sample(size):\n",
    "    sample_z  = noise.sample(size)\n",
    "    feed_dict = {z: sample_z.reshape((-1, 1))}\n",
    "    return sess.run(G, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2fVQUtl1xXY"
   },
   "outputs": [],
   "source": [
    "# Compare data and GAN distributions\n",
    "def plot_distributions_2d(data, noise, step, title, data2=None, data3=None, data4=None):\n",
    "    # Number of samples\n",
    "    num_samples = 5000\n",
    "\n",
    "    # Data\n",
    "    x_data = data.sample(num_samples)\n",
    "\n",
    "    if(data2):\n",
    "      x_data2 = data2.sample(num_samples)\n",
    "    if(data3):\n",
    "      x_data3 = data3.sample(num_samples) \n",
    "    if(data4):\n",
    "      x_data4 = data4.sample(num_samples) \n",
    "\n",
    "    # Generated samples\n",
    "    x_gan = sample(num_samples)\n",
    "\n",
    "    # Plot distributions\n",
    "    plt.scatter(x_data[:,0], x_data[:,1], c='blue', alpha=0.3, label='Data distribution')\n",
    "\n",
    "    if(data2):\n",
    "      plt.scatter(x_data2[:,0], x_data2[:,1], c='blue', alpha=0.3)\n",
    "    if(data3):\n",
    "      plt.scatter(x_data3[:,0], x_data3[:,1], c='blue', alpha=0.3)    \n",
    "    if(data4):\n",
    "      plt.scatter(x_data4[:,0], x_data4[:,1], c='blue', alpha=0.3)\n",
    "    \n",
    "    plt.scatter(x_gan[:,0], x_gan[:,1],  c='red', alpha=0.3, label='GAN distribution')\n",
    "      \n",
    "    # Plot decision boundary\n",
    "    #y = decision_boundary(bin_centers)\n",
    "    #plt.plot(bin_centers, y, color='orange', label='Decision boundary')\n",
    "\n",
    "    # Set limits\n",
    "    plt.xlim(-2, 10)\n",
    "    plt.ylim(-2, 10)\n",
    "\n",
    "    # Legend\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Axis labels\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Save figure\n",
    "    plt.savefig('bivariate_normal_gan_'+str(step)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUHSBRK0vS8H"
   },
   "outputs": [],
   "source": [
    "# Seed the TF random number generator for reproducible initialization\n",
    "tf.set_random_seed(seed)\n",
    "# Seed random number generator\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001 #0.0001 for 1-d\n",
    "num_steps     = 10000\n",
    "batch_size    = 20\n",
    "check_every   = 1000\n",
    "noise_bound   = 16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "Xmo3X4gd3idR",
    "outputId": "fefc505e-d8d8-4349-98da-a242d60f55b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables\n",
      "---------\n",
      "G/hidden_1/W:0 (1, 4)\n",
      "G/hidden_1/b:0 (4,)\n",
      "G/hidden_2/W:0 (4, 4)\n",
      "G/hidden_2/b:0 (4,)\n",
      "G/output/W:0 (4, 2)\n",
      "G/output/b:0 (2,)\n",
      "D/hidden_1/W:0 (2, 8)\n",
      "D/hidden_1/b:0 (8,)\n",
      "D/hidden_2/W:0 (8, 8)\n",
      "D/hidden_2/b:0 (8,)\n",
      "D/logits/W:0 (8, 1)\n",
      "D/logits/b:0 (1,)\n",
      "=> Total number of parameters = 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melike Nur Mermer\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Train 2D DATA\n",
    "# Define GAN\n",
    "\n",
    "# Generator\n",
    "with tf.variable_scope('G') as scope:\n",
    "    z = tf.placeholder(tf.float32, [None, 1])\n",
    "    G = generator2d(z)\n",
    "    G_variables = get_variables(scope)\n",
    "\n",
    "# Discriminator\n",
    "with tf.variable_scope('D') as scope:\n",
    "    x  = tf.placeholder(tf.float32, [None, 2])\n",
    "    D1 = discriminator2d(x)\n",
    "    D_variables = get_variables(scope)\n",
    "\n",
    "    # Copy of the discriminator that receives generator samples\n",
    "    scope.reuse_variables()\n",
    "    D2 = discriminator2d(G) #G.shape=2 D1.shape=1\n",
    "\n",
    "# Losses\n",
    "D_loss = -tf.reduce_mean(tf.log(D1) + tf.log(1 - D2))\n",
    "G_loss = -tf.reduce_mean(tf.log(D2))\n",
    "\n",
    "# Train ops\n",
    "D_train_op = optimizer(D_loss, D_variables, learning_rate)\n",
    "G_train_op = optimizer(G_loss, G_variables, learning_rate)\n",
    "\n",
    "\n",
    "# Print list of variables\n",
    "print(\"Variables\")\n",
    "print(\"---------\")\n",
    "variables  = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "num_params = 0\n",
    "for v in variables:\n",
    "    num_params += np.prod(v.get_shape().as_list())\n",
    "    print(v.name, v.get_shape())\n",
    "print(\"=> Total number of parameters =\", num_params)\n",
    "\n",
    "# TF session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Data distribution\n",
    "data = DataDistribution2d()\n",
    "data2 = DataDistribution2d()\n",
    "data2.change(mu=([6, 2]), sigma =([[.25, 1], [.5,.25]]))\n",
    "data3 = DataDistribution2d()\n",
    "data3.change(mu=([7, 7]), sigma =([[.1, .001], [.001, .01]]))\n",
    "data4 = DataDistribution2d()\n",
    "data4.change(mu=([1, 1]), sigma =([[1, -1], [1, -1]])) # mu=([6,2])\n",
    "#data2=None\n",
    "#data3=None\n",
    "#data4=None\n",
    "\n",
    "# Noise distribution\n",
    "noise = NoiseDistribution()\n",
    "#noise.change(mu=([8, 8]), sigma =([[.25, 1], [.5,.25]]))\n",
    "\n",
    "# Train\n",
    "for step in range(num_steps):\n",
    "    # Update discriminator\n",
    "    batch_x   = data.sample(batch_size) #her epochda dağılımdan batch_size tane örnek üretiyor, belli bir örnek kümesinden örnek seçmiyor!\n",
    "    if(data2):\n",
    "      batch_x2   = data2.sample(batch_size)\n",
    "      batch_x   = np.concatenate((batch_x, batch_x2), axis=0)\n",
    "    if(data3):\n",
    "      batch_x3   = data3.sample(batch_size)\n",
    "      batch_x   = np.concatenate((batch_x, batch_x3), axis=0)    \n",
    "    if(data4):\n",
    "      batch_x4   = data4.sample(batch_size)\n",
    "      batch_x   = np.concatenate((batch_x, batch_x4), axis=0)\n",
    "\n",
    "    batch_z   = noise.sample(4*batch_size, sort=True) #2 dist ise 2*batch_size\n",
    "\n",
    "    feed_dict = {x: batch_x, z: batch_z.reshape((-1, 1))} # dağılımlardan bu adımda ürettiğimiz örnekleri daha önceden tanımladığımız placeholderlara atıyoruz\n",
    "    _, current_D_loss = sess.run([D_train_op, D_loss], feed_dict) \n",
    "\n",
    "    # Update generator\n",
    "    batch_z   = noise.sample(4*batch_size) #başka bir fake data batchi ile generator lossu hesaplanıyor\n",
    "    feed_dict = {z: batch_z.reshape((-1, 1))} #bu batchi dict şeklinde tutuyoruz\n",
    "    _, current_G_loss = sess.run([G_train_op, G_loss], feed_dict)\n",
    "\n",
    "    \n",
    "    # Progress report\n",
    "    if (step+1) % check_every == 0:\n",
    "        title=str(\"After \" + str(step+1) + \" steps: \\n Discriminator loss=\" + str(round(current_D_loss, 4)) + \"  Generator loss=\" + str(round(current_G_loss, 4)))\n",
    "        plot_distributions_2d(data, noise, int((step+1)/check_every), title,  data2, data3, data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dfmvaOdYfZ3"
   },
   "outputs": [],
   "source": [
    "noise=NoiseDistribution(16)\n",
    "x_gan=noise.sample(500, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "uaZFHaW8Gt0L",
    "outputId": "70c49f94-5cd8-40e3-83e3-2eb84098d5b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11cf7dd7a48>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-16, 16, 101) # (-bound, +bound) aralığını 101 parçaya bölüyor\n",
    "bin_centers = (bins[:-1] + bins[1:])/2\n",
    "p_gan, _ = np.histogram(x_gan, bins=bins, density=True)\n",
    "plt.plot(bin_centers, p_gan,  c='red', alpha=0.3, label='GAN distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qu8uFvaDIBym"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2D_GAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
